<!doctype html>
<html class="no-js" lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Capturing semantic structures in Neural Machine Translation</title>

    <link rel="stylesheet" type="text/css" href="http://localhost:4000/assets/css/styles_feeling_responsive.css">

  

	<script src="http://localhost:4000/assets/js/modernizr.min.js"></script>

	<script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script>
	<script>
		WebFont.load({
			google: {
				families: [ 'Lato:400,700,400italic:latin', 'Volkhov::latin' ]
			}
		});
	</script>

	<noscript>
		<link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic%7CVolkhov' rel='stylesheet' type='text/css'>
	</noscript>


	<!-- Search Engine Optimization -->
	<meta name="description" content="This project is aimed towards people who are interested in ML and Neuro-Linguistic Programming; and handles Neural Machine Translation with an unusual approach


Neural Networks are being extensively used for machine translation. While tools like Google Translate might look so practically effective in translation, the accuracy (BLEU scores) of such models on test benchmarks is just in the range of 28-40%! From our initial experiments with such models, we can say that these networks essentially learn to output translations all of which carry the same semantic structure – a different way of referring to the same thing or paraphrasing. There has been some previous work in the domain of learning to align to fixed semantic structure or learning to handle paraphrasing during NMT. We have a simple approach in mind, which we want to try out.



We stick to the standard Encoder-Decoder architecture for Neural Machine Translation(NMT). To give a brief overview, a recurrent neural network (Encoder) takes the input sequence and converts it to an intermediate hidden representation on which another recurrent neural network (Decoder) operates and produces the output sequence. The standard NMT architecture involves a single encoder and decoder mechanism. Here, essentially, we want to have multiple decoders, each of which can capture a different semantic structure of the sequence. Training these multiple decoders will involve not only maximising the likelihood for prediction for each but also producing diverse outputs among themselves. If successfully learned, this model can learn to produce translations which are different ways of saying the same thing.



The motivation for this idea comes from an analogy to generative models like GANs. Often GANs suffer from mode collapse, that is they fail to capture different modes in the space of images, and rotate among different modes. Having a ‘n-way’ mixture (which essentially is implemented by multiple decoders), we can prevent such a problem from arising.



Some (initial) relevant papers:

  Adversarial Example Generation with Syntactically Controlled Paraphrase Networks https://arxiv.org/pdf/1804.06059.pdf
  Diverse Beam Search https://arxiv.org/abs/1610.02424
  Breaking softmax bottleneck (might be interesting to see if their solution captures semantic structures too)  https://arxiv.org/abs/1711.03953



Base Code: Tensorflow Official NMT repository  https://github.com/tensorflow/nmt


Why should one choose this project?
This project is more on the lines of exploration so active participation is needed from your side not only for the coding part but also for formulating the problem and thus you will have the freedom to test and implement your own ideas as well. This will be an excellent way to venture into the field of Machine Learning (Deep Learning in specific) and can act as a solid starting point for you to pursue more projects/research in DL. This project will also involve active reading of research papers and hence can be a very hands on research experience for you.


Who should choose this project?
A prerequisite for choosing this project is having a basic knowledge of Neural Networks and concepts of Machine Learning (Though this can also be learnt while doing the project if student is motivated and interested to learn). The student should be comfortable coding in Python and should have ability to learn to program in a new environment (Tensorflow). Any prior experience with Keras, Pytorch or Tensorflow is appreciated. 



  
    
      Week
      Work
    
  
  
    
      Week 1 :
      Reading Seq2Seq NMT paper and getting familiarized with the code by trying out some simple variations and trying to replicate benchmarks on some datasets
    
    
      Week 2 onwards:
      Implementing a multi-decoder module in the NMT code and modifying the loss for training by incorporating cross entropy on the predicted sequence and divergence between different decoder models
    
    
      .
      Testing initially on synthetic datasets for proof of concept and use this to tweak the model architecture, loss, etc to improve the prediction scores. If there are signs of the model being able to capture wider semantic structures, we can test the model on bigger datasets.
    
  



Miscellaneous
This project might be resource intensive. Google Cloud or other Cloud platforms might be required to test full blown models, once the model seems to work on toy data.


If the idea works out and results are encouraging, this might lead to an academic research paper at an NLP/ML conference.">
	<meta name="google-site-verification" content="Vk0IOJ2jwG_qEoG7fuEXYqv0m2rLa8P778Fi_GrsgEQ">
	<meta name="msvalidate.01" content="0FB4C028ABCF07C908C54386ABD2D97F" >
	
	<link rel="author" href="https://plus.google.com/u/0/118311555303973066167">
	
	
	<link rel="canonical" href="http://localhost:4000/soc_projects/08-aviral-siddhant-semantics/">


	<!-- Facebook Open Graph -->
	<meta property="og:title" content="Capturing semantic structures in Neural Machine Translation">
	<meta property="og:description" content="This project is aimed towards people who are interested in ML and Neuro-Linguistic Programming; and handles Neural Machine Translation with an unusual approach


Neural Networks are being extensively used for machine translation. While tools like Google Translate might look so practically effective in translation, the accuracy (BLEU scores) of such models on test benchmarks is just in the range of 28-40%! From our initial experiments with such models, we can say that these networks essentially learn to output translations all of which carry the same semantic structure – a different way of referring to the same thing or paraphrasing. There has been some previous work in the domain of learning to align to fixed semantic structure or learning to handle paraphrasing during NMT. We have a simple approach in mind, which we want to try out.



We stick to the standard Encoder-Decoder architecture for Neural Machine Translation(NMT). To give a brief overview, a recurrent neural network (Encoder) takes the input sequence and converts it to an intermediate hidden representation on which another recurrent neural network (Decoder) operates and produces the output sequence. The standard NMT architecture involves a single encoder and decoder mechanism. Here, essentially, we want to have multiple decoders, each of which can capture a different semantic structure of the sequence. Training these multiple decoders will involve not only maximising the likelihood for prediction for each but also producing diverse outputs among themselves. If successfully learned, this model can learn to produce translations which are different ways of saying the same thing.



The motivation for this idea comes from an analogy to generative models like GANs. Often GANs suffer from mode collapse, that is they fail to capture different modes in the space of images, and rotate among different modes. Having a ‘n-way’ mixture (which essentially is implemented by multiple decoders), we can prevent such a problem from arising.



Some (initial) relevant papers:

  Adversarial Example Generation with Syntactically Controlled Paraphrase Networks https://arxiv.org/pdf/1804.06059.pdf
  Diverse Beam Search https://arxiv.org/abs/1610.02424
  Breaking softmax bottleneck (might be interesting to see if their solution captures semantic structures too)  https://arxiv.org/abs/1711.03953



Base Code: Tensorflow Official NMT repository  https://github.com/tensorflow/nmt


Why should one choose this project?
This project is more on the lines of exploration so active participation is needed from your side not only for the coding part but also for formulating the problem and thus you will have the freedom to test and implement your own ideas as well. This will be an excellent way to venture into the field of Machine Learning (Deep Learning in specific) and can act as a solid starting point for you to pursue more projects/research in DL. This project will also involve active reading of research papers and hence can be a very hands on research experience for you.


Who should choose this project?
A prerequisite for choosing this project is having a basic knowledge of Neural Networks and concepts of Machine Learning (Though this can also be learnt while doing the project if student is motivated and interested to learn). The student should be comfortable coding in Python and should have ability to learn to program in a new environment (Tensorflow). Any prior experience with Keras, Pytorch or Tensorflow is appreciated. 



  
    
      Week
      Work
    
  
  
    
      Week 1 :
      Reading Seq2Seq NMT paper and getting familiarized with the code by trying out some simple variations and trying to replicate benchmarks on some datasets
    
    
      Week 2 onwards:
      Implementing a multi-decoder module in the NMT code and modifying the loss for training by incorporating cross entropy on the predicted sequence and divergence between different decoder models
    
    
      .
      Testing initially on synthetic datasets for proof of concept and use this to tweak the model architecture, loss, etc to improve the prediction scores. If there are signs of the model being able to capture wider semantic structures, we can test the model on bigger datasets.
    
  



Miscellaneous
This project might be resource intensive. Google Cloud or other Cloud platforms might be required to test full blown models, once the model seems to work on toy data.


If the idea works out and results are encouraging, this might lead to an academic research paper at an NLP/ML conference.">
	<meta property="og:url" content="http://localhost:4000/soc_projects/08-aviral-siddhant-semantics/">
	<meta property="og:locale" content="en_EN">
	<meta property="og:type" content="website">
	<meta property="og:site_name" content="WnCC">
	
	<meta property="article:author" content="https://www.facebook.com/thenotsodarkknight">


	
	<!-- Twitter -->
	<meta name="twitter:card" content="summary">
	<meta name="twitter:site" content="awairforitbeen">
	<meta name="twitter:creator" content="awairforitbeen">
	<meta name="twitter:title" content="Capturing semantic structures in Neural Machine Translation">
	<meta name="twitter:description" content="This project is aimed towards people who are interested in ML and Neuro-Linguistic Programming; and handles Neural Machine Translation with an unusual approach


Neural Networks are being extensively used for machine translation. While tools like Google Translate might look so practically effective in translation, the accuracy (BLEU scores) of such models on test benchmarks is just in the range of 28-40%! From our initial experiments with such models, we can say that these networks essentially learn to output translations all of which carry the same semantic structure – a different way of referring to the same thing or paraphrasing. There has been some previous work in the domain of learning to align to fixed semantic structure or learning to handle paraphrasing during NMT. We have a simple approach in mind, which we want to try out.



We stick to the standard Encoder-Decoder architecture for Neural Machine Translation(NMT). To give a brief overview, a recurrent neural network (Encoder) takes the input sequence and converts it to an intermediate hidden representation on which another recurrent neural network (Decoder) operates and produces the output sequence. The standard NMT architecture involves a single encoder and decoder mechanism. Here, essentially, we want to have multiple decoders, each of which can capture a different semantic structure of the sequence. Training these multiple decoders will involve not only maximising the likelihood for prediction for each but also producing diverse outputs among themselves. If successfully learned, this model can learn to produce translations which are different ways of saying the same thing.



The motivation for this idea comes from an analogy to generative models like GANs. Often GANs suffer from mode collapse, that is they fail to capture different modes in the space of images, and rotate among different modes. Having a ‘n-way’ mixture (which essentially is implemented by multiple decoders), we can prevent such a problem from arising.



Some (initial) relevant papers:

  Adversarial Example Generation with Syntactically Controlled Paraphrase Networks https://arxiv.org/pdf/1804.06059.pdf
  Diverse Beam Search https://arxiv.org/abs/1610.02424
  Breaking softmax bottleneck (might be interesting to see if their solution captures semantic structures too)  https://arxiv.org/abs/1711.03953



Base Code: Tensorflow Official NMT repository  https://github.com/tensorflow/nmt


Why should one choose this project?
This project is more on the lines of exploration so active participation is needed from your side not only for the coding part but also for formulating the problem and thus you will have the freedom to test and implement your own ideas as well. This will be an excellent way to venture into the field of Machine Learning (Deep Learning in specific) and can act as a solid starting point for you to pursue more projects/research in DL. This project will also involve active reading of research papers and hence can be a very hands on research experience for you.


Who should choose this project?
A prerequisite for choosing this project is having a basic knowledge of Neural Networks and concepts of Machine Learning (Though this can also be learnt while doing the project if student is motivated and interested to learn). The student should be comfortable coding in Python and should have ability to learn to program in a new environment (Tensorflow). Any prior experience with Keras, Pytorch or Tensorflow is appreciated. 



  
    
      Week
      Work
    
  
  
    
      Week 1 :
      Reading Seq2Seq NMT paper and getting familiarized with the code by trying out some simple variations and trying to replicate benchmarks on some datasets
    
    
      Week 2 onwards:
      Implementing a multi-decoder module in the NMT code and modifying the loss for training by incorporating cross entropy on the predicted sequence and divergence between different decoder models
    
    
      .
      Testing initially on synthetic datasets for proof of concept and use this to tweak the model architecture, loss, etc to improve the prediction scores. If there are signs of the model being able to capture wider semantic structures, we can test the model on bigger datasets.
    
  



Miscellaneous
This project might be resource intensive. Google Cloud or other Cloud platforms might be required to test full blown models, once the model seems to work on toy data.


If the idea works out and results are encouraging, this might lead to an academic research paper at an NLP/ML conference.">
	
	

	<link type="text/plain" rel="author" href="http://localhost:4000/humans.txt">

	

	

	<link rel="icon" sizes="32x32" href="http://localhost:4000/assets/img/favicon-32x32.png">

	<link rel="icon" sizes="192x192" href="http://localhost:4000/assets/img/touch-icon-192x192.png">

	<link rel="apple-touch-icon-precomposed" sizes="180x180" href="http://localhost:4000/assets/img/apple-touch-icon-180x180-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="152x152" href="http://localhost:4000/assets/img/apple-touch-icon-152x152-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/assets/img/apple-touch-icon-144x144-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="120x120" href="http://localhost:4000/assets/img/apple-touch-icon-120x120-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000/assets/img/apple-touch-icon-114x114-precomposed.png">

	
	<link rel="apple-touch-icon-precomposed" sizes="76x76" href="http://localhost:4000/assets/img/apple-touch-icon-76x76-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/assets/img/apple-touch-icon-72x72-precomposed.png">

	<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/assets/img/apple-touch-icon-precomposed.png">	

	<meta name="msapplication-TileImage" content="http://localhost:4000/assets/img/msapplication_tileimage.png">

	<meta name="msapplication-TileColor" content="#fabb00">


	

</head>
<body id="top-of-page" class="">
	
	
<div id="navigation" class="sticky">
  <nav class="top-bar" role="navigation" data-topbar>
    <ul class="title-area">
      <li class="name">
      <h1 class="show-for-small-only"><a href="http://localhost:4000" class="icon-tree"> WnCC</a></h1>
    </li>
       <!-- Remove the class "menu-icon" to get rid of menu icon. Take out "Menu" to just have icon alone -->
      <li class="toggle-topbar menu-icon"><a href="#"><span>Nav</span></a></li>
    </ul>
    <section class="top-bar-section">

      <ul class="right">
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
            
            
              <li class="divider"></li>
              <li><a  href="http://localhost:4000/search/">Search</a></li>

            
            
          
        

              

          
          
            
            
              <li class="divider"></li>
              <li><a  href="http://localhost:4000/contact/">Contact</a></li>

            
            
          
        
        
      </ul>

      <ul class="left">
        

              

          
          

            
            
              <li><a  href="http://localhost:4000/">WnCC</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          

            
            
              <li><a  href="https://portals.wncc-iitb.org/internships/login/" target="_blank">Internship Portal</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          

            
            
              <li><a  href="https://wncc-iitb.org/wiki/index.php/The_Web_and_Coding_Club" target="_blank">Wiki</a></li>
              <li class="divider"></li>

            
            
          
        

              

          
          

            
            

              <li class="has-dropdown">
                <a  href="http://localhost:4000">Events</a>

                  <ul class="dropdown">
                    

                      

                      <li><a  href="http://localhost:4000/events/18/">2018</a></li>
                    

                      

                      <li><a  href="http://localhost:4000/events/17/">2017</a></li>
                    

                      

                      <li><a  href="http://localhost:4000/events/16/">2016</a></li>
                    

                      

                      <li><a  href="http://localhost:4000/events/15/">2015</a></li>
                    

                      

                      <li><a  href="http://localhost:4000/events/14/">2014</a></li>
                    

                      

                      <li><a  href="http://localhost:4000/events/13/">2013</a></li>
                    
                  </ul>

              </li>
              <li class="divider"></li>
            
          
        

              

          
          

            
            

              <li class="has-dropdown">
                <a  href="http://localhost:4000/soc/">Seasons of Code</a>

                  <ul class="dropdown">
                    

                      

                      <li><a  href="http://localhost:4000/soc/18/">2018</a></li>
                    

                      

                      <li><a  href="http://localhost:4000/soc/17/">2017</a></li>
                    
                  </ul>

              </li>
              <li class="divider"></li>
            
          
        

              

          
          

            
            

              <li class="has-dropdown">
                <a  href="http://localhost:4000/team/">The Team</a>

                  <ul class="dropdown">
                    

                      

                      <li><a  href="http://localhost:4000/team/18-19/">2018-19</a></li>
                    

                      

                      <li><a  href="http://localhost:4000/team/17-18/">2017-18</a></li>
                    

                      

                      <li><a  href="http://localhost:4000/team/16-17/">2016-17</a></li>
                    

                      

                      <li><a  href="http://localhost:4000/team/15-16/">2015-16</a></li>
                    

                      

                      <li><a  href="http://localhost:4000/team/14-15/">2014-15</a></li>
                    

                      

                      <li><a  href="http://localhost:4000/team/13-14/">2013-14</a></li>
                    
                  </ul>

              </li>
              <li class="divider"></li>
            
          
        

              

          
          
        

              

          
          
        
        
      </ul>
    </section>
  </nav>
</div><!-- /#navigation -->

	

	

<div id="masthead-no-image-header">
	<div class="row">
		<div class="small-12 columns">
			<a id="logo" href="http://localhost:4000/" title="WnCC – The Web and Coding Club">
				<img src="http://localhost:4000/assets/img/wncc.png" alt="WnCC – The Web and Coding Club">
			</a>
		</div><!-- /.small-12.columns -->
	</div><!-- /.row -->
</div><!-- /#masthead -->








	<!-- Main -->
<div class="row t30">
    <div class="medium-8 columns">
        <article itemscope itemtype="http://schema.org/Article">
            <header>
                
                <figure>
                    <img src="../..//images/semantics.png" width="970" alt="Capturing semantic structures in Neural Machine Translation" itemprop="image">

                    
                </figure>
                

                <div itemprop="name">
                    <p class="subheadline">Machine Learning, NLP</p>
                    <h1>Capturing semantic structures in Neural Machine Translation</h1>
                    <p class="subheadline">Aviral KumarSiddhant Garg<br>
                    Stipend: INR 3000 
                    </p>
                </div>
            </header>


            

            <div itemprop="articleSection">
            <hr />
<h4 id="this-project-is-aimed-towards-people-who-are-interested-in-ml-and-neuro-linguistic-programming-and-handles-neural-machine-translation-with-an-unusual-approach">This project is aimed towards people who are interested in ML and Neuro-Linguistic Programming; and handles Neural Machine Translation with an unusual approach</h4>
<!--break-->

<p>Neural Networks are being extensively used for machine translation. While tools like Google Translate might look so practically effective in translation, the accuracy (BLEU scores) of such models on test benchmarks is just in the range of 28-40%! From our initial experiments with such models, we can say that these networks essentially learn to output translations all of which carry the same semantic structure – a different way of referring to the same thing or paraphrasing. There has been some previous work in the domain of learning to align to fixed semantic structure or learning to handle paraphrasing during NMT. We have a simple approach in mind, which we want to try out.</p>

<!--break-->

<p>We stick to the standard Encoder-Decoder architecture for Neural Machine Translation(NMT). To give a brief overview, a recurrent neural network (Encoder) takes the input sequence and converts it to an intermediate hidden representation on which another recurrent neural network (Decoder) operates and produces the output sequence. The standard NMT architecture involves a single encoder and decoder mechanism. Here, essentially, we want to have multiple decoders, each of which can capture a different semantic structure of the sequence. Training these multiple decoders will involve not only maximising the likelihood for prediction for each but also producing diverse outputs among themselves. If successfully learned, this model can learn to produce translations which are different ways of saying the same thing.</p>

<!--break-->

<p>The motivation for this idea comes from an analogy to generative models like GANs. Often GANs suffer from mode collapse, that is they fail to capture different modes in the space of images, and rotate among different modes. Having a ‘n-way’ mixture (which essentially is implemented by multiple decoders), we can prevent such a problem from arising.</p>

<!--break-->

<h4 id="some-initial-relevant-papers">Some (initial) relevant papers:</h4>
<ol>
  <li>Adversarial Example Generation with Syntactically Controlled Paraphrase Networks <a href="https://arxiv.org/pdf/1804.06059.pdf">https://arxiv.org/pdf/1804.06059.pdf</a></li>
  <li>Diverse Beam Search <a href="https://arxiv.org/abs/1610.02424">https://arxiv.org/abs/1610.02424</a></li>
  <li>Breaking softmax bottleneck (might be interesting to see if their solution captures semantic structures too)  <a href="https://arxiv.org/abs/1711.03953">https://arxiv.org/abs/1711.03953</a></li>
</ol>

<!--break-->
<h4 id="base-code-tensorflow-official-nmt-repository--httpsgithubcomtensorflownmt">Base Code: Tensorflow Official NMT repository  <a href="https://github.com/tensorflow/nmt">https://github.com/tensorflow/nmt</a></h4>
<!--break-->

<h4 id="why-should-one-choose-this-project">Why should one choose this project?</h4>
<p>This project is more on the lines of exploration so active participation is needed from your side not only for the coding part but also for formulating the problem and thus you will have the freedom to test and implement your own ideas as well. This will be an excellent way to venture into the field of Machine Learning (Deep Learning in specific) and can act as a solid starting point for you to pursue more projects/research in DL. This project will also involve active reading of research papers and hence can be a very hands on research experience for you.
<!--break--></p>

<h4 id="who-should-choose-this-project">Who should choose this project?</h4>
<p>A prerequisite for choosing this project is having a basic knowledge of Neural Networks and concepts of Machine Learning (Though this can also be learnt while doing the project if student is motivated and interested to learn). The student should be comfortable coding in Python and should have ability to learn to program in a new environment (Tensorflow). Any prior experience with Keras, Pytorch or Tensorflow is appreciated. 
<!--break--></p>

<table>
  <thead>
    <tr>
      <th>Week</th>
      <th>Work</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Week 1 :</td>
      <td>Reading Seq2Seq NMT paper and getting familiarized with the code by trying out some simple variations and trying to replicate benchmarks on some datasets</td>
    </tr>
    <tr>
      <td>Week 2 onwards:</td>
      <td>Implementing a multi-decoder module in the NMT code and modifying the loss for training by incorporating cross entropy on the predicted sequence and divergence between different decoder models</td>
    </tr>
    <tr>
      <td>.</td>
      <td>Testing initially on synthetic datasets for proof of concept and use this to tweak the model architecture, loss, etc to improve the prediction scores. If there are signs of the model being able to capture wider semantic structures, we can test the model on bigger datasets.</td>
    </tr>
  </tbody>
</table>

<!--break-->
<h4 id="miscellaneous">Miscellaneous</h4>
<p>This project might be resource intensive. Google Cloud or other Cloud platforms might be required to test full blown models, once the model seems to work on toy data.</p>

<!--break-->
<p>If the idea works out and results are encouraging, this might lead to an academic research paper at an NLP/ML conference.</p>

            </div>

            <!--<div itemprop="articleSection">
                <p class="subheadline">
                    Selected Mentees: Abhijeet DubeyKrishna WadhwaniGobinath Balamurugan <br>
                    How to Contact the Mentor: <a target="_blank" >Email ID</a> - aviralkumar2907@gmail.com, sid7954@gmail.com
                </p>   
            </div>-->

            

            
            <br>
            <br>
            
        </article>
    </div><!-- /.medium-8.columns -->


    


    
    <div class="medium-4 columns">
        <aside>
	<div class="panel radius">
		<h3>Seasons of Code</h3>
		<p>
			Seasons of Code is a programme launched by the WnCC, along the lines of GSoC without much greenery though. The incentive is similar to ITSP, based on the current form of it, the fundamental difference is that one can choose from the ideas offered by mentors who are senior undergrads, doctorate students or professors, and in some exceptional cases, startups. We plan to have a really long timeframe though, until the next winter extending this programme into a mentorship of sorts into the semester. It is not just about development by the way. We have some mentors ready to take up programmes regarding competitive coding and scientific computation too. 
		</p>
	</div>

	<div class="panel radius">
		<h3>List of SoC Projects</h3><br>
    <p align = "center">
            <a class="button large radius" href="../../soc/17" ><font style="font-size: '30'">Seasons of Code 2017</font></a> <br> <a class="button large radius" href="../../soc/18">Seasons of Code 2018</a>
    </p>
    </div>

	<div class="border-dotted radius b30">
		<img src="http://placekitten.com/271/270" alt="uh, Placekitten">
		<p class="text-left">
			This is an advertisment with a crazy cat! <a href="http://placekitten.com/">More cats, please!</a>
		</p>
	</div>
</aside>
    </div><!-- /.medium-4.columns -->
    


</div><!-- /.row -->


	
	    <div id="up-to-top" class="row">
      <div class="small-12 columns" style="text-align: right;">
        <a class="iconfont" href="#top-of-page">&#xf108;</a>
      </div><!-- /.small-12.columns -->
    </div><!-- /.row -->


    <footer id="footer-content" class="bg-grau">
      <div id="footer">
        <div class="row">
          <div class="medium-6 large-5 columns">
            <h5 class="shadow-black">About This Site</h5>

            <p class="shadow-black">
              We're WnCC, the Web and Coding Club of IIT Bombay. We code and help people code. Being the chief coding organisation of one of the most sought after technical institutions in the country, we have some of the very best programmers working with us. This page enlists our ongoing activities. It is maintained by the coding community at IIT-B. We're wncc-iitb on most services. Also, we love XKCD.
              <a href="http://localhost:4000/info/">More ›</a>
            </p>
          </div><!-- /.large-6.columns -->


          <div class="small-6 medium-3 large-3 large-offset-1 columns">
            
              
                <h5 class="shadow-black">Services</h5>
              
            
              
            
              
            
              
            
              
            

              <ul class="no-bullet shadow-black">
              
                
                  <li >
                    <a href="http://localhost:4000"  title=""></a>
                  </li>
              
                
                  <li >
                    <a href="http://localhost:4000/contact/"  title="Contact">Contact</a>
                  </li>
              
                
                  <li >
                    <a href="http://localhost:4000/feed.xml"  title="Subscribe to RSS Feed">RSS</a>
                  </li>
              
                
                  <li >
                    <a href="http://localhost:4000/atom.xml"  title="Subscribe to Atom Feed">Atom</a>
                  </li>
              
                
                  <li >
                    <a href="http://localhost:4000/sitemap.xml"  title="Sitemap for Google Webmaster Tools">sitemap.xml</a>
                  </li>
              
              </ul>
          </div><!-- /.large-4.columns -->


          <div class="small-6 medium-3 large-3 columns">
            
              
                <h5 class="shadow-black">Assets</h5>
              
            
              
            
              
            
              
            

            <ul class="no-bullet shadow-black">
            
              
                <li >
                  <a href="http://localhost:4000"  title=""></a>
                </li>
            
              
                <li class="network-entypo" >
                  <a href="https://wncc-iitb.org/wiki/index.php/The_Web_and_Coding_Club" target="_blank"  title="Wiki">Wiki</a>
                </li>
            
              
                <li class="services-newsletter" >
                  <a href="https://portals.wncc-iitb.org/internships/login/" target="_blank"  title="Internship Portal">Internship Portal</a>
                </li>
            
              
                <li class="rss-link" >
                  <a href="https://groups.google.com/forum/#!forum/wncc_iitb" target="_blank"  title="WnCC Google Group">WnCC Google Group</a>
                </li>
            
            </ul>
          </div><!-- /.large-3.columns -->
        </div><!-- /.row -->

      </div><!-- /#footer -->


      <div id="subfooter">
        <nav class="row">
          <section id="subfooter-left" class="small-12 medium-6 columns credits">
            <p>Created with &hearts; by <a href="http://www.facebook.com/thenotsodarkknight">Abeen Bhattacharya</a></p>
          </section>

          <section id="subfooter-right" class="small-12 medium-6 columns">
            <ul class="inline-list social-icons">
            
              <li><a href="http://github.com/thenotsodarkknight" target="_blank" class="icon-github" title="Code und mehr..."></a></li>
            
              <li><a href="http://www.youtube.com/wncc" target="_blank" class="icon-youtube" title="Videos, Video-Anleitungen und Filme von Phlow auf YouTube"></a></li>
            
              <li><a href="http://twitter.com/awaitforitbeen" target="_blank" class="icon-twitter" title="Immer das Neuste von Phlow gibt es auf Twitter"></a></li>
            
              <li><a href="http://www.mixcloud.com/phlow/" target="_blank" class="icon-cloud" title="Mixe, was sonst?"></a></li>
            
              <li><a href="https://plus.google.com/u/0/+Phlow" target="_blank" class="icon-googleplus" title="YouTube Google+"></a></li>
            
              <li><a href="http://www.facebook.com/thenotsodarkknight" target="_blank" class="icon-facebook" title="facebook"></a></li>
            
              <li><a href="http://instagram.com/lazybaccha" target="_blank" class="icon-instagram" title=""></a></li>
            
            </ul>
          </section>
        </nav>
      </div><!-- /#subfooter -->
    </footer>

	

	


<script src="http://localhost:4000/assets/js/javascript.min.js"></script>














</body>
</html>

